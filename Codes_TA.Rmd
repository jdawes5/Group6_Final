---
title: 'Group 6 Final Project: Codes and Technical Analysis'
author: "Junran Cao, Jordan Dawes, and Carter Rogers"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
    number_sections: true
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=F}
library(dplyr)
library(ggplot2)
library(tidyr)
library(gridExtra)
library(tidyverse)  
library(corrplot)
library(ezids)
library(httr)
library(randomForest)
library(tree)
library(caret)
library(broom)
```

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3, scipen = 999) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# Loading and cleaning data

```{r}
dataset <- read.csv("~/Downloads/US_Accidents_Dec20_updated.csv")
dataset <- na.omit(dataset) #335,552 observations remaining
```

```{r}
#encoding correct datatypes
dataset$ID <- as.character(dataset$ID)
dataset$Severity <- as.factor(dataset$Severity)
dataset$Start_Time <- as.character(dataset$Start_Time)
dataset$End_Time <- as.character(dataset$End_Time)
dataset$Description <- as.character(dataset$Description)
dataset$Street <- as.character(dataset$Street)
dataset$City <- as.character(dataset$City)
dataset$County <- as.character(dataset$County)
dataset$State <- as.character(dataset$State)
dataset$Zipcode <- as.numeric(dataset$Zipcode)
dataset$Airport_Code <- as.character(dataset$Airport_Code)
dataset$Weather_Timestamp <- as.character(dataset$Weather_Timestamp)
dataset$Weather_Condition <- as.character(dataset$Weather_Condition)
```

```{r}
#remove unnecessary columns
dataset <- subset(dataset, select = -c(Number, Street, Side, Airport_Code, Weather_Timestamp, Start_Lat, Start_Lng, End_Lat, End_Lng))
```

# EDA

```{r}
table(dataset$Severity) #vast majority are severity 2, why?
table(dataset$State) #states w/large pop have most accidents, no surprises there
ggplot(data = dataset, aes(x = Severity, y = Distance.mi.)) + geom_boxplot() #looks wacky with most distances = 0
ggplot(data = dataset, aes(x = Distance.mi.)) + geom_histogram(binwidth = .05) + scale_x_continuous(limits = c(0, 10)) #may need to omit distances of 0
ggplot(data = dataset, aes(x = Temperature.F., y = Wind_Speed.mph., colour = Severity)) + geom_point() + scale_y_continuous(limits = c(0, 50))
```




# Is there a relationship between Civil Twilight and Accident Occurences? Is there any relationship between visibility and the seeverity of an accident?

```{r}

loadPkg("dplyr")
loadPkg("ggplot2")

#Renaming relevant columns
dataset <- rename(dataset, c("civ.light" = "Civil_Twilight"),c("vis" = "Visibility.mi."))

#Subsetting out blank values for the variable civ.light
visdata <- subset(dataset, civ.light != "")

#Converting civ.light to a factor variable
visdata$civ.light <- as.factor(visdata$civ.light)

#qq plot to see if visibilit is normally distributed
qqnorm(visdata$vis)
qqline(visdata$vis)

#Bar plot to show day time vs night time accidents as well as the severity levels that make up each
ggplot(visdata, aes(y=vis, x=civ.light, fill= Severity)) +
  geom_bar(stat='identity') +
  labs(title="Total Accidents Based on Daylight",
       x="Daylight Status",
       y="Number of Accidents",
       fill="Daylight Status")

#Histogram to show 
ggplot(data = visdata, aes(vis)) + geom_histogram(mapping = aes(x = vis), binwidth = 1)+labs(title = "Histogram for Accident Count Based on Visibility", x = "Vsibility (miles)", y = "Count")

```



```{r}
#use one way anova to test severity and visibility
#chi squared test for severity and civ.light

#ANOVA used to explore relationship between severity and visibility
anovatest <- aov(vis ~ Severity, data = visdata)
anovatest
xkabledply(anovatest)
anovasummary<- summary(anovatest)


#Chi-square to explore Severity based on Civil Twilight
chitable <- table(visdata$Severity, visdata$civ.light)
testchi <- chisq.test(chitable)
testchi
testchi$p.value
```
Anova test on visibility and severity results in a p-value of `r anovasummary[[1]][["Pr(>F)"]][[1]]`

The chi-square test reveals a p-value of `r testchi$p.value`

Is this enough to predict the severity of an accident? Most likely not but we can explore it anyways.

```{r results='markup'}
loadPkg("MASS")

vislogit <- polr(Severity ~ vis+civ.light, data = visdata, Hess = "TRUE")
vislogit
summary(vislogit)

vistable <- coef(summary(vislogit))

coefs = exp(coef(vislogit))
xkabledply( as.table(coefs), title = "Exponentials of Coefficients for Model" )

vispredict = predict(vislogit, visdata)
table(visdata$Severity, vispredict)


loadPkg("pscl")
visaic <- AIC(vislogit)
visaic
vispr <- pR2(vislogit)
vispr


unloadPkg("pscl")
unloadPkg("MASS")

```



# Can we classify the severity of a given accident based on temperature, wind speed, or other weather conditions present during the accident?

```{r}
#try decision trees and random forests instead

set.seed(123)

newdata2 <- subset(dataset, select = c(Severity, Temperature.F., Wind_Speed.mph., Wind_Chill.F., Humidity..., Pressure.in., vis, Precipitation.in.))

#scale vars
scaleddata2 <- as.data.frame(scale(newdata2[2:8], center = TRUE, scale = TRUE))
scaleddata2 <- cbind(newdata2[,1], scaleddata2)
names(scaleddata2)[1] <- 'Severity'

trainIndex2 <- createDataPartition(scaleddata2$Severity, p = .7, list = FALSE, times = 1)
carter_train2 <- scaleddata2[trainIndex2,]
carter_test2 <- scaleddata2[-trainIndex2,]

rf_model2 <- randomForest(Severity ~ .
                    , data = carter_train2
                    , mtry = 3
                    , ntree = 20
                    , importance = TRUE
                    , type = "prob")

predicted_table <- predict(rf_model2, carter_test2[,-1])
table(observed = carter_test2[,1], predicted = predicted_table)
print(rf_model2)
round(importance(rf_model2), 2)

#stratified sampling
rf_model3 <- randomForest(Severity ~ .
                    , data = carter_train2
                    , mtry = 3
                    , ntree = 20
                    , importance = TRUE
                    , type = "prob"
                    , sampsize = c(1000, 1000, 1000, 1000))

predicted_table2 <- predict(rf_model3, carter_test2[,-1])
table(observed = carter_test2[,1], predicted = predicted_table2)
print(rf_model3)
round(importance(rf_model3), 2)
```


#What factors best predict the severity of an accident?

```{r}
#remove all cols except numeric, factor, and binary. Also excluding Distance since this is not a "predictive" metric but rather a rough measurement of severity
rf_data = subset(dataset, select = -c(ID, Start_Time, End_Time, Distance.mi., Description, City, County, State, Zipcode, Country, Weather_Condition))

#split train-test
rf_trainIndex <- createDataPartition(rf_data$Severity, p = .7, list = FALSE, times = 1)
rf_carter_train <- rf_data[rf_trainIndex,]
rf_carter_test <- rf_data[-rf_trainIndex,]

rf_model <- randomForest(Severity ~ .
                    , data = rf_carter_train
                    , mtry = 5
                    , ntree = 20
                    , importance = TRUE
                    , type = "prob")

varImpPlot(rf_model, n.var = 5, main = "Variable Importance Plot for Random Forest Model (Severity)")
#wind direction, humidity, temperature appear in both plots
```

# Is there any relationship between the severity of an accident and the road conditions (traffic stops, railways, junctions, etc.)?


Preprocessing and cleaning
```{r}
df=subset(dataset,select=c(Severity, Amenity,Bump, Crossing,Give_Way, Junction, No_Exit, Railway, Roundabout,Station, Stop, Traffic_Calming, Traffic_Signal, Turning_Loop))
str(df)
df=as.data.frame(df)
df$Amenity = factor(df$Amenity)
df$Bump=factor(df$Bump)
df$Crossing=factor(df$Crossing)
df$Give_Way=factor(df$Give_Way)
df$Junction=factor(df$Junction)
df$No_Exit=factor(df$No_Exit)
df$Railway=factor(df$Railway)
df$Roundabout=factor(df$Roundabout)
df$Station=factor(df$Station)
df$Stop=factor(df$Stop)
df$Traffic_Calming=factor(df$Traffic_Calming)
df$Traffic_Signal=factor(df$Traffic_Signal)
df$Turning_Loop=factor(df$Turning_Loop)
df$Severity=as.factor(df$Severity)
head(df)
```
Note: After reviewing the structure of the data set, we found out that "Turning_Loop" only had one level, meaning that the values of it were all "False".Therefore, we excluded it from our analysis for now. 

Analysis - Logistic Regression

```{r}
loadPkg("MASS")
dfLogit <- polr(Severity ~ Amenity+Bump+Crossing+Give_Way+Junction+No_Exit+Railway+Roundabout+Station+Stop+Traffic_Calming+Traffic_Signal, data = df, Hess = "TRUE")
dfLogit
summary(dfLogit)
(ctable <- coef(summary(dfLogit)))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p))
xkabledply( confint.default(dfLogit), title = "CIs using standard errors" )
expcoeff = exp(coef(dfLogit))
xkabledply( as.table(expcoeff), title = "Exponential of coefficients in Logit Reg" )
```
All the coefficient were significant (small p-values)

If the 95% CI does not cross 0, the parameter estimate can be statistically significant.Therefore, "Bump", "Crossing","No-Exit","Stop",'Traffic_Signal", "Roundabout", and "Station" were not statistically significant.

Interpretation:

When there was an amenity, the odds of the accident being more severe (i.e.,4, 3,2 versus 1) was 1.2886 times that of the accidents which had no amenity, holding constant all other variables.

When there was a give-way, the odds of the accident being more severe (i.e.,4, 3,2 versus 1) was 1.6544 times that of the accidents which had no give-way, holding constant all other variables.

When there was a junction, the odds of the accident being more severe (i.e.,4, 3,2 versus 1) was 2.0765 times that of the accidents which had no junction, holding constant all other variables.

When there was railway, the odds of the accident being more severe (i.e.,4, 3,2 versus 1) was 1.7134 times that of the accidents which had no railway, holding constant all other variables.

When there was traffic-calming, the odds of the accident being more severe (i.e.,4, 3,2 versus 1) was 2.3826 times that of the accidents which had no traffic-traffic-calming, holding constant all other variables.

Model Evaluation

```{r}
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
admitLogitpr2 = pR2(dfLogit)
admitLogitpr2
unloadPkg("pscl") 
```
According to the McFadden statistics, only 1.36% of the variations in y is explained by the explanatory variables in the model. This result urged us to look at other variables.